# -tica-IA
# RelatÃ³rio â€“ Dilema Ã‰tico em IA: Reconhecimento Facial

ğŸ‘‰ Um algoritmo pode reforÃ§ar desigualdades?  
Analisamos o dilema Ã©tico do **reconhecimento facial**.

---

## ğŸ“Œ O Problema
Sistemas de reconhecimento facial, usados por polÃ­cias e empresas, apresentam **erros desproporcionais** ao identificar pessoas negras e mulheres.  
Estudos do MIT apontaram que a taxa de falhas pode ser atÃ© **100 vezes maior** em comparaÃ§Ã£o com rostos brancos.  
AlÃ©m disso, hÃ¡ riscos graves de **violaÃ§Ã£o da privacidade** e de uso abusivo para vigilÃ¢ncia em massa.

---

## ğŸ” Nossa AnÃ¡lise
- O sistema Ã© uma **â€œcaixa pretaâ€**, sem transparÃªncia sobre como decisÃµes sÃ£o tomadas.  
- HÃ¡ um **viÃ©s algorÃ­tmico e de dados**: bancos de imagens pouco diversos treinam modelos que reproduzem desigualdades.  
- Impactos sociais incluem **prisÃµes injustas, reforÃ§o do racismo estrutural** e ameaÃ§a a direitos fundamentais garantidos pela **LGPD**.  

---

## ğŸ§­ Nosso Posicionamento
O uso irrestrito de reconhecimento facial na seguranÃ§a pÃºblica deve ser **limitado ou banido**.  

Recomendamos que:  
1. AdoÃ§Ã£o de **auditorias independentes** para avaliar viÃ©s antes do uso.  
2. CriaÃ§Ã£o de **leis especÃ­ficas** para regulamentar seu emprego em seguranÃ§a.  
3. Uso restrito a contextos com **supervisÃ£o humana e transparÃªncia**.  

ğŸ’¡ A tecnologia pode ser inovadora, mas nÃ£o Ã  custa da justiÃ§a social.  
Como profissionais, precisamos garantir que a IA respeite direitos fundamentais.

---

## ğŸ“‚ RelatÃ³rio Completo
ğŸ‘‰ [Clique aqui para acessar o PDF da anÃ¡lise]

---

### ğŸ”– Hashtags
#Ã‰ticaEmIA #ReconhecimentoFacial #ViÃ©sAlgorÃ­tmico #LGPD #JustiÃ§aTecnolÃ³gica
